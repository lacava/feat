{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Feat Feat is a feature engineering automation tool that learns new representations of raw data to improve classifier and regressor performance. The underlying methods use Pareto optimization and evolutionary computation to search the space of possible transformations. Feat wraps around a user-chosen ML method and provides a set of representations that give the best performance for that method. Each individual in Feat s population is its own data representation. Feat uses the Shogun C++ ML toolbox to fit models. Check out the documentation for installation and examples. Cite La Cava, W., Singh, T. R., Taggart, J., Suri, S., Moore, J. H. (2018). Learning concise representations for regression by evolving networks of trees. arxiv:1807.0091 Bibtex: @article{la_cava_learning_2018, title = {Learning concise representations for regression by evolving networks of trees}, url = {https://arxiv.org/abs/1807.00981}, language = {en}, author = {La Cava, William and Singh, Tilak Raj and Taggart, James and Suri, Srinivas and Moore, Jason H.}, month = jul, year = {2018} } Acknowledgments This method is being developed to study human disease in the Epistasis Lab at UPenn . License GNU GPLv3","title":"Introduction"},{"location":"#feat","text":"Feat is a feature engineering automation tool that learns new representations of raw data to improve classifier and regressor performance. The underlying methods use Pareto optimization and evolutionary computation to search the space of possible transformations. Feat wraps around a user-chosen ML method and provides a set of representations that give the best performance for that method. Each individual in Feat s population is its own data representation. Feat uses the Shogun C++ ML toolbox to fit models. Check out the documentation for installation and examples.","title":"Feat"},{"location":"#cite","text":"La Cava, W., Singh, T. R., Taggart, J., Suri, S., Moore, J. H. (2018). Learning concise representations for regression by evolving networks of trees. arxiv:1807.0091 Bibtex: @article{la_cava_learning_2018, title = {Learning concise representations for regression by evolving networks of trees}, url = {https://arxiv.org/abs/1807.00981}, language = {en}, author = {La Cava, William and Singh, Tilak Raj and Taggart, James and Suri, Srinivas and Moore, Jason H.}, month = jul, year = {2018} }","title":"Cite"},{"location":"#acknowledgments","text":"This method is being developed to study human disease in the Epistasis Lab at UPenn .","title":"Acknowledgments"},{"location":"#license","text":"GNU GPLv3","title":"License"},{"location":"api_c/","text":"Feat C++ API This is the C++ API for Feat . Back to main documentation Quick links Feat Parameters Evaluation Selection Variation Population","title":"Feat C++ API"},{"location":"api_c/#feat-c-api","text":"This is the C++ API for Feat . Back to main documentation","title":"Feat C++ API"},{"location":"api_c/#quick-links","text":"Feat Parameters Evaluation Selection Variation Population","title":"Quick links"},{"location":"contributing/","text":"Contributing Please follow the Github flow guidelines for contributing to this project. In general, this is the approach: Fork the repo into your own repository and clone it locally. git clone https://github.com/my_user_name/feat Have an idea for a code change. Checkout a new branch with an appropriate name. git checkout -b my_new_change Make your changes. Commit your changes to the branch. git commit -m adds my new change Check that your branch has no conflict with Feat s master branch by merging the master branch from the upstream repo. git remote add upstream https://github.com/lacava/feat git fetch upstream git merge upstream/master Fix any conflicts and commit. git commit -m Merges upstream master Push the branch to your forked repo. git push origin my_new_change Go to either Github repo and make a new Pull Request for your forked branch. Be sure to reference any relevant issues.","title":"Contribute"},{"location":"contributing/#contributing","text":"Please follow the Github flow guidelines for contributing to this project. In general, this is the approach: Fork the repo into your own repository and clone it locally. git clone https://github.com/my_user_name/feat Have an idea for a code change. Checkout a new branch with an appropriate name. git checkout -b my_new_change Make your changes. Commit your changes to the branch. git commit -m adds my new change Check that your branch has no conflict with Feat s master branch by merging the master branch from the upstream repo. git remote add upstream https://github.com/lacava/feat git fetch upstream git merge upstream/master Fix any conflicts and commit. git commit -m Merges upstream master Push the branch to your forked repo. git push origin my_new_change Go to either Github repo and make a new Pull Request for your forked branch. Be sure to reference any relevant issues.","title":"Contributing"},{"location":"install/","text":"Installation To see our installation process from scratch, check out the Travis install file . Feat depends on the Eigen matrix library for C++ as well as the Shogun ML library. Both come in easy packages that work across platforms. If you need Eigen and Shogun, follow the instructions in Dependencies . Feat uses cmake to build. It uses the typical set of instructions: git clone https://github.com/lacava/feat # clone the repo cd feat # enter the directory ./configure # this runs mkdir build; cd build; cmake .. ./install # this runs make -C build VERBOSE=1 -j8 Python wrapper The python wrapper is installed using setuptools as follows: cd python python setup.py install Dependencies Eigen Eigen is a header only package. We need Eigen 3 or greater. Debian/Ubuntu On Debian systems, you can grab the package: sudo apt-get install libeigen3-dev You can also download the headers and put them somewhere. Then you just have to tell cmake where they are with the environmental variable EIGEN3_INCLUDE_DIR . Example: # grab Eigen 3.3.4 wget http://bitbucket.org/eigen/eigen/get/3.3.4.tar.gz tar xzf 3 .3.4.tar.gz mkdir eigen-3.3.4 mv eigen-eigen*/* eigen-3.3.4 # set an environmental variable to tell cmake where Eigen is export EIGEN3_INCLUDE_DIR = $( pwd ) /eigen-3.3.4/ Shogun You don t have to compile Shogun, just download the binaries. Their install guide is good. We ve listed two of the options here. Anaconda A good option for Anaconda users is the Shogun Anaconda package. If you use conda, you can get what you need by conda install -c conda-forge shogun-cpp If you do this, you need tell cmake where to find Anaconda s library and include directories. Set these two variables appropriately: export SHOGUN_LIB=/home/travis/miniconda/lib/ export SHOGUN_DIR=/home/travis/miniconda/include/ Debian/Ubuntu You can also get the Shogun packages: sudo add-apt-repository ppa:shogun-toolbox/nightly -y sudo apt-get update -y sudo apt-get install -qq --force-yes --no-install-recommends libshogun18 sudo apt-get install -qq --force-yes --no-install-recommends libshogun-dev Running the tests (optional) If you want to run the tests, you need to install Google Test . A useful guide to doing so is available here . Then you can use cmake to build the tests. From the repo root, ./configure tests # builds the test Makefile make -C build tests # compiles the tests ./build/tests # runs the tests","title":"Installation"},{"location":"install/#installation","text":"To see our installation process from scratch, check out the Travis install file . Feat depends on the Eigen matrix library for C++ as well as the Shogun ML library. Both come in easy packages that work across platforms. If you need Eigen and Shogun, follow the instructions in Dependencies . Feat uses cmake to build. It uses the typical set of instructions: git clone https://github.com/lacava/feat # clone the repo cd feat # enter the directory ./configure # this runs mkdir build; cd build; cmake .. ./install # this runs make -C build VERBOSE=1 -j8","title":"Installation"},{"location":"install/#python-wrapper","text":"The python wrapper is installed using setuptools as follows: cd python python setup.py install","title":"Python wrapper"},{"location":"install/#dependencies","text":"","title":"Dependencies"},{"location":"install/#eigen","text":"Eigen is a header only package. We need Eigen 3 or greater.","title":"Eigen"},{"location":"install/#debianubuntu","text":"On Debian systems, you can grab the package: sudo apt-get install libeigen3-dev You can also download the headers and put them somewhere. Then you just have to tell cmake where they are with the environmental variable EIGEN3_INCLUDE_DIR . Example: # grab Eigen 3.3.4 wget http://bitbucket.org/eigen/eigen/get/3.3.4.tar.gz tar xzf 3 .3.4.tar.gz mkdir eigen-3.3.4 mv eigen-eigen*/* eigen-3.3.4 # set an environmental variable to tell cmake where Eigen is export EIGEN3_INCLUDE_DIR = $( pwd ) /eigen-3.3.4/","title":"Debian/Ubuntu"},{"location":"install/#shogun","text":"You don t have to compile Shogun, just download the binaries. Their install guide is good. We ve listed two of the options here.","title":"Shogun"},{"location":"install/#anaconda","text":"A good option for Anaconda users is the Shogun Anaconda package. If you use conda, you can get what you need by conda install -c conda-forge shogun-cpp If you do this, you need tell cmake where to find Anaconda s library and include directories. Set these two variables appropriately: export SHOGUN_LIB=/home/travis/miniconda/lib/ export SHOGUN_DIR=/home/travis/miniconda/include/","title":"Anaconda"},{"location":"install/#debianubuntu_1","text":"You can also get the Shogun packages: sudo add-apt-repository ppa:shogun-toolbox/nightly -y sudo apt-get update -y sudo apt-get install -qq --force-yes --no-install-recommends libshogun18 sudo apt-get install -qq --force-yes --no-install-recommends libshogun-dev","title":"Debian/Ubuntu"},{"location":"install/#running-the-tests","text":"(optional) If you want to run the tests, you need to install Google Test . A useful guide to doing so is available here . Then you can use cmake to build the tests. From the repo root, ./configure tests # builds the test Makefile make -C build tests # compiles the tests ./build/tests # runs the tests","title":"Running the tests"},{"location":"api/python/api_py/","text":"Coming soon. For now, refer to the source.","title":"Python"},{"location":"examples/archive/","text":"Using the Archive In this example, we apply Feat to a regression problem and visualize the archive of representations. Note: this code uses the Penn ML Benchmark Suite to fetch data. You can install it using pip install pmlb . Also available as a notebook Training Feat First, we import the data and create a train-test split. from pmlb import fetch_data from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error as mse import numpy as np dataset = 690_visualizing_galaxy X , y = fetch_data ( dataset , return_X_y = True ) X_t , X_v , y_t , y_v = train_test_split ( X , y , train_size = 0.75 , test_size = 0.25 , random_state = 42 ) Then we set up a Feat instance and train the model, storing the final archive. from feat import Feat # fix the random state random_state = 11314 fest = Feat ( pop_size = 500 , # train 500 representations gens = 100 , # maximum of 200 generations max_time = 60 , # max time of 1 minute ml = LinearRidgeRegression , # use ridge regression (the default) sel = lexicase , # use epsilon lexicase selection (the default) surv = nsga2 , # use nsga-2 survival (the defaut) max_depth = 6 , # constrain features to depth of 6 max_dim = min ([ X . shape [ 1 ] * 2 , 50 ]), # limit representation dimensionality random_state = random_state , backprop = True , # use gradient descent to optimize weights iters = 10 , n_threads = 4 , # max 1 threads verbosity = 2 , # verbose output logfile = feat_ + dataset + .log , # save a log file of the training loss print_pop = 1 # print the final population ) # train the model fest . fit ( X_t , y_t ) # get the test score test_score = {} test_score [ feat ] = mse ( y_v , fest . predict ( X_v )) # store the archive str_arc = fest . get_archive () For comparison, we can fit an Elastic Net and Random Forest regression model to the same data: # random forest rf = RandomForestRegressor ( random_state = 987039487 ) rf . fit ( X_t , y_t ) test_score [ rf ] = mse ( y_v , rf . predict ( X_v )) # elastic net linest = ElasticNet () linest . fit ( X_t , y_t ) # test_score={} test_score [ elasticnet ] = mse ( y_v , linest . predict ( X_v )) Visualizing the Archive Let s visualize this archive with the test scores. This gives us a sense of how increasing the representation complexity affects the quality of the model and its generalization. import numpy as np import matplotlib import matplotlib.pyplot as plt import seaborn as sns matplotlib . rcParams [ figure.figsize ] = ( 10 , 6 ) % matplotlib inline sns . set_style ( white ) complexity = [] fit_train = [] fit_val = [] fit_test = [] eqn = [] h = plt . figure ( figsize = ( 10 , 6 )) # store archive data from string for s in str_arc . split ( \\n )[ 1 : - 1 ]: line = s . split ( \\t ) complexity . append ( int ( line [ 0 ])) fit_train . append ( float ( line [ 1 ])) fit_test . append ( float ( line [ 2 ])) eqn . append ( , . join ( line [ 3 :])) eqn [ - 1 ] . replace ( sqrt , \\sqrt ) # plot archive points plt . plot ( fit_train , complexity , --ro , label = Train , markersize = 6 ) plt . plot ( fit_test , complexity , --bx , label = Validation ) best = np . argmin ( np . array ( fit_test )) print ( best: , complexity [ best ]) plt . plot ( fit_test [ best ], complexity [ best ], sk , markersize = 16 , markerfacecolor = none , label = Model Selection ) # test score lines y1 = - 1 y2 = 58 plt . plot (( test_score [ feat ], test_score [ feat ]),( y1 , y2 ), --k , label = FEAT Test , alpha = 0.5 ) plt . plot (( test_score [ rf ], test_score [ rf ]),( y1 , y2 ), -.xg , label = RF Test , alpha = 0.5 ) plt . plot (( test_score [ elasticnet ], test_score [ elasticnet ]),( y1 , y2 ), -sm , label = ElasticNet Test , alpha = 0.5 ) print ( complexity , complexity ) eqn [ best ] = 0)]$ \\n $ . join ( eqn [ best ] . split ( 0)] )) # complexity[-1] = complexity[-1]-10 # adjust placement of last equation xoff = 70 for e , t , c in zip ( eqn , fit_test , complexity ): if c in [ 1 , 5 , 12 , 31 , 43 , 53 ]: if c == 5 or c == 1 : t = t + 200 if c == complexity [ best ]: tax = plt . text ( t + 18000 , c - 5 , $\\leftarrow + e + $ , size = 18 , horizontalalignment = right ) tax . set_bbox ( dict ( facecolor = white , alpha = 0.75 , edgecolor = none )) elif c == 43 : plt . text ( t + xoff , c - 1 , $\\leftarrow$ overfitting , size = 18 ) else : tax = plt . text ( t + xoff , c - 1 , $\\leftarrow + e + $ , size = 18 ) tax . set_bbox ( dict ( facecolor = white , alpha = 0.75 , edgecolor = none )) l = plt . legend ( prop = { size : 16 }, loc = [ 0.72 , 0.05 ]) plt . xlabel ( MSE , size = 16 ) # plt.gca().set_ylim(10,200) plt . gca () . set_xlim ( 150 , right = 20000 ) # plt.gca().set_yscale( log ) plt . gca () . set_xscale ( log ) # plt.ylim(y1,y2) plt . gca () . set_yticklabels ( ) plt . gca () . set_xticklabels ( ) plt . ylabel ( Complexity , size = 18 ) h . tight_layout () h . savefig ( archive_example.svg ) plt . show () This produces the figure below. Note that ElasticNet produces a similar test score to the linear representation in Feat s archive, and that Random Forest s test score is near the representation $[(x_2 + x_0)][(x1-x_3)][\\sin(x)]$. The best model, marked with a square, is selected from the validation curve (blue line). The validation curve shows how models begin to overfit as complexity grows. By visualizing the archive, we can see that a lower complexity model (the representation beginning with $relu(x_3)$) achieves nearly as good of a validation score. In this case it may be preferable to choose that representation instead. We can also see that the validation scores in general oscillate around the test score of the final model.","title":"Using the Archive"},{"location":"examples/archive/#using-the-archive","text":"In this example, we apply Feat to a regression problem and visualize the archive of representations. Note: this code uses the Penn ML Benchmark Suite to fetch data. You can install it using pip install pmlb . Also available as a notebook","title":"Using the Archive"},{"location":"examples/archive/#training-feat","text":"First, we import the data and create a train-test split. from pmlb import fetch_data from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error as mse import numpy as np dataset = 690_visualizing_galaxy X , y = fetch_data ( dataset , return_X_y = True ) X_t , X_v , y_t , y_v = train_test_split ( X , y , train_size = 0.75 , test_size = 0.25 , random_state = 42 ) Then we set up a Feat instance and train the model, storing the final archive. from feat import Feat # fix the random state random_state = 11314 fest = Feat ( pop_size = 500 , # train 500 representations gens = 100 , # maximum of 200 generations max_time = 60 , # max time of 1 minute ml = LinearRidgeRegression , # use ridge regression (the default) sel = lexicase , # use epsilon lexicase selection (the default) surv = nsga2 , # use nsga-2 survival (the defaut) max_depth = 6 , # constrain features to depth of 6 max_dim = min ([ X . shape [ 1 ] * 2 , 50 ]), # limit representation dimensionality random_state = random_state , backprop = True , # use gradient descent to optimize weights iters = 10 , n_threads = 4 , # max 1 threads verbosity = 2 , # verbose output logfile = feat_ + dataset + .log , # save a log file of the training loss print_pop = 1 # print the final population ) # train the model fest . fit ( X_t , y_t ) # get the test score test_score = {} test_score [ feat ] = mse ( y_v , fest . predict ( X_v )) # store the archive str_arc = fest . get_archive () For comparison, we can fit an Elastic Net and Random Forest regression model to the same data: # random forest rf = RandomForestRegressor ( random_state = 987039487 ) rf . fit ( X_t , y_t ) test_score [ rf ] = mse ( y_v , rf . predict ( X_v )) # elastic net linest = ElasticNet () linest . fit ( X_t , y_t ) # test_score={} test_score [ elasticnet ] = mse ( y_v , linest . predict ( X_v ))","title":"Training Feat"},{"location":"examples/archive/#visualizing-the-archive","text":"Let s visualize this archive with the test scores. This gives us a sense of how increasing the representation complexity affects the quality of the model and its generalization. import numpy as np import matplotlib import matplotlib.pyplot as plt import seaborn as sns matplotlib . rcParams [ figure.figsize ] = ( 10 , 6 ) % matplotlib inline sns . set_style ( white ) complexity = [] fit_train = [] fit_val = [] fit_test = [] eqn = [] h = plt . figure ( figsize = ( 10 , 6 )) # store archive data from string for s in str_arc . split ( \\n )[ 1 : - 1 ]: line = s . split ( \\t ) complexity . append ( int ( line [ 0 ])) fit_train . append ( float ( line [ 1 ])) fit_test . append ( float ( line [ 2 ])) eqn . append ( , . join ( line [ 3 :])) eqn [ - 1 ] . replace ( sqrt , \\sqrt ) # plot archive points plt . plot ( fit_train , complexity , --ro , label = Train , markersize = 6 ) plt . plot ( fit_test , complexity , --bx , label = Validation ) best = np . argmin ( np . array ( fit_test )) print ( best: , complexity [ best ]) plt . plot ( fit_test [ best ], complexity [ best ], sk , markersize = 16 , markerfacecolor = none , label = Model Selection ) # test score lines y1 = - 1 y2 = 58 plt . plot (( test_score [ feat ], test_score [ feat ]),( y1 , y2 ), --k , label = FEAT Test , alpha = 0.5 ) plt . plot (( test_score [ rf ], test_score [ rf ]),( y1 , y2 ), -.xg , label = RF Test , alpha = 0.5 ) plt . plot (( test_score [ elasticnet ], test_score [ elasticnet ]),( y1 , y2 ), -sm , label = ElasticNet Test , alpha = 0.5 ) print ( complexity , complexity ) eqn [ best ] = 0)]$ \\n $ . join ( eqn [ best ] . split ( 0)] )) # complexity[-1] = complexity[-1]-10 # adjust placement of last equation xoff = 70 for e , t , c in zip ( eqn , fit_test , complexity ): if c in [ 1 , 5 , 12 , 31 , 43 , 53 ]: if c == 5 or c == 1 : t = t + 200 if c == complexity [ best ]: tax = plt . text ( t + 18000 , c - 5 , $\\leftarrow + e + $ , size = 18 , horizontalalignment = right ) tax . set_bbox ( dict ( facecolor = white , alpha = 0.75 , edgecolor = none )) elif c == 43 : plt . text ( t + xoff , c - 1 , $\\leftarrow$ overfitting , size = 18 ) else : tax = plt . text ( t + xoff , c - 1 , $\\leftarrow + e + $ , size = 18 ) tax . set_bbox ( dict ( facecolor = white , alpha = 0.75 , edgecolor = none )) l = plt . legend ( prop = { size : 16 }, loc = [ 0.72 , 0.05 ]) plt . xlabel ( MSE , size = 16 ) # plt.gca().set_ylim(10,200) plt . gca () . set_xlim ( 150 , right = 20000 ) # plt.gca().set_yscale( log ) plt . gca () . set_xscale ( log ) # plt.ylim(y1,y2) plt . gca () . set_yticklabels ( ) plt . gca () . set_xticklabels ( ) plt . ylabel ( Complexity , size = 18 ) h . tight_layout () h . savefig ( archive_example.svg ) plt . show () This produces the figure below. Note that ElasticNet produces a similar test score to the linear representation in Feat s archive, and that Random Forest s test score is near the representation $[(x_2 + x_0)][(x1-x_3)][\\sin(x)]$. The best model, marked with a square, is selected from the validation curve (blue line). The validation curve shows how models begin to overfit as complexity grows. By visualizing the archive, we can see that a lower complexity model (the representation beginning with $relu(x_3)$) achieves nearly as good of a validation score. In this case it may be preferable to choose that representation instead. We can also see that the validation scores in general oscillate around the test score of the final model.","title":"Visualizing the Archive"},{"location":"examples/command_line/","text":"Command line example Feat can be run from the command-line. All of its options are configurable there. After a default build, the feat executable will be in the build directory. From the repo directory, type ./build/feat -h to see options. The first argument to the executable should be the dataset file to learn from. This dataset should be a comma- or tab-delimited file with columns corresponding to features, one column corresponding to the target, and rows corresponding to samples. the first row should be a header with the names of the features and target. The target must be named as class, target, or label in order to be interpreted correctly. See the datasets in the examples folder for guidance. ENC problem We will run Feat on the energy efficiency dataset from UCI, which is included in docs/examples/data/d_enc.txt . To run Feat with a population 1000 for 100 generations using a random seed of 42, type ./build/feat docs/examples/d_enc.csv -p 100 -g 100 -r 42 The default verbosity=1, so you will get a printout of summary statistics each generation. The final output should look like Generation 100/100 [//////////////////////////////////////////////////] Min Loss Median Loss Median (Max) Size Time (s) 2.74477e+00 4.40906e+00 12 (20) 3.91275 Representation Pareto Front-------------------------------------- Rank Complexity Loss Representation 1 1 1.53447e+01 [x4] 1 2 1.22824e+01 [x6][x4] 1 3 9.66792e+00 [x2][x6][x4] 1 4 9.20641e+00 [x0][x3][x4][x6] 1 5 9.07840e+00 [x0][x1][x3][x4][x6] 1 6 7.29209e+00 [relu(x2)][x4][x6] 1 7 7.26300e+00 [relu(x2)][x3][x4][x6] 1 8 5.92479e+00 [relu(x2)][x6][x0][x1][x3] 1 9 5.71534e+00 [relu(x2)][x4][x0][x1][x6][x3] 1 10 5.71534e+00 [relu(x2)][x4][x0][x1][x6][x3][x1] 1 11 5.71534e+00 [relu(x2)][x4][x0][x1][x6][x3][x0][x1] 1 12 5.06225e+00 [relu(x2)][x4][x6][(x2*x1)] 1 13 4.94241e+00 [relu(x2)][x4][x1][x6][(x2*x1)] 1 14 4.48444e+00 [relu(x2)][x4][x0][x1][x6][(x2*x1)] 1 15 4.48433e+00 [relu(x2)][x4][x0][x1][x6][x3][(x2*x1)] 1 16 4.46352e+00 [relu(x2)][x4][x0][x1][x6][(x2*x1)][float(x5)] 1 18 4.42668e+00 [relu(x2)][x4][x0][x1][x6][sqrt(|x6|)][(x2*x1)] 1 19 4.39144e+00 [relu(x2)][x4][x0][x1][(x6^3)][(x2*x1)] 1 20 4.27968e+00 [relu(x2)][x4][x0][x1][x6][(x6^3)][(x2*x1)] 1 21 4.05328e+00 [relu(x2)][sqrt(|x0|)][x6][x3][x4][(x1*x0)][(x0^2)] 1 22 3.85010e+00 [relu(x2)][sqrt(|x0|)][x6][x3][(x2/x4)][(x1*x0)] 1 23 3.22447e+00 [relu(x2)][x0][sqrt(|x0|)][x6][(x2/x4)][x2][(x1*x0)] 1 24 3.15101e+00 [relu(x2)][x4][x0][sqrt(|x0|)][x6][(x2/x4)][x2][(x1*x0)] 1 26 3.14630e+00 [relu(x2)][x4][x0][sqrt(|x0|)][x6][(x2/x4)][x2][(x1*x0)][flo... 1 28 3.12080e+00 [relu(x2)][x4][x0][x6][(x2/x4)][x2][(x1*x0)][log(x0)] 1 29 3.07627e+00 [relu(x2)][x0][sqrt(|x0|)][x6][(x2/x4)][x2][(x1*x0)][(x6^3)]... 1 32 3.05042e+00 [relu(x2)][x4][x0][x6][(x2/x4)][x2][(x1*x0)][log(x0)][sqrt(|... 1 33 3.04806e+00 [relu(x2)][x4][x3][x0][x6][(x2/x4)][x2][(x1*x0)][log(x0)][sq... 1 38 2.92064e+00 [relu(x2)][x0][sqrt(|x6|)][x6][(x2/x4)][x2][(x1*x0)][log(x0)... 1 45 2.79345e+00 [relu(x2)][x0][x6][(x2/x4)][x2][(x1*x0)][log(x0)][(x6)^(x3)]... 1 46 2.78956e+00 [relu(x2)][x0][x3][x6][(x2/x4)][x2][(x1*x0)][log(x0)][(x6)^(... 1 49 2.74477e+00 [relu(x2)][x0][sqrt(|x6|)][x6][(x2/x4)][x2][(x1*x0)][log(x0)... finished best training representation: [relu(x2)][x0][sqrt(|x6|)][x6][(x2/x4)][x2][(x1*x0)][log(x0)][(x6)^(x3)][(x6^3)] train score: 2.744773 updating best.. best validation representation: [relu(x2)][x0][x6][(x2/x4)][x2][(x1*x0)][log(x0)][(x6)^(x3)][(x6^3)] validation score: 2.907846 final_model score: 2.795017 generating training prediction... train score: 2.7950e+00 train r2: 9.6791e-01 generating test prediction... test score: 2.8997e+00 test r2: 9.7102e-01 printing final model Feature Weight x0 370.995875 (x1*x0) 202.241059 x2 159.910905 relu(x2) -105.321384 log(x0) 101.855423 (x6)^(x3) 85.811398 (x2/x4) 37.755580 x6 35.792342 (x6^3) 25.457528 done!","title":"Command line"},{"location":"examples/command_line/#command-line-example","text":"Feat can be run from the command-line. All of its options are configurable there. After a default build, the feat executable will be in the build directory. From the repo directory, type ./build/feat -h to see options. The first argument to the executable should be the dataset file to learn from. This dataset should be a comma- or tab-delimited file with columns corresponding to features, one column corresponding to the target, and rows corresponding to samples. the first row should be a header with the names of the features and target. The target must be named as class, target, or label in order to be interpreted correctly. See the datasets in the examples folder for guidance.","title":"Command line example"},{"location":"examples/command_line/#enc-problem","text":"We will run Feat on the energy efficiency dataset from UCI, which is included in docs/examples/data/d_enc.txt . To run Feat with a population 1000 for 100 generations using a random seed of 42, type ./build/feat docs/examples/d_enc.csv -p 100 -g 100 -r 42 The default verbosity=1, so you will get a printout of summary statistics each generation. The final output should look like Generation 100/100 [//////////////////////////////////////////////////] Min Loss Median Loss Median (Max) Size Time (s) 2.74477e+00 4.40906e+00 12 (20) 3.91275 Representation Pareto Front-------------------------------------- Rank Complexity Loss Representation 1 1 1.53447e+01 [x4] 1 2 1.22824e+01 [x6][x4] 1 3 9.66792e+00 [x2][x6][x4] 1 4 9.20641e+00 [x0][x3][x4][x6] 1 5 9.07840e+00 [x0][x1][x3][x4][x6] 1 6 7.29209e+00 [relu(x2)][x4][x6] 1 7 7.26300e+00 [relu(x2)][x3][x4][x6] 1 8 5.92479e+00 [relu(x2)][x6][x0][x1][x3] 1 9 5.71534e+00 [relu(x2)][x4][x0][x1][x6][x3] 1 10 5.71534e+00 [relu(x2)][x4][x0][x1][x6][x3][x1] 1 11 5.71534e+00 [relu(x2)][x4][x0][x1][x6][x3][x0][x1] 1 12 5.06225e+00 [relu(x2)][x4][x6][(x2*x1)] 1 13 4.94241e+00 [relu(x2)][x4][x1][x6][(x2*x1)] 1 14 4.48444e+00 [relu(x2)][x4][x0][x1][x6][(x2*x1)] 1 15 4.48433e+00 [relu(x2)][x4][x0][x1][x6][x3][(x2*x1)] 1 16 4.46352e+00 [relu(x2)][x4][x0][x1][x6][(x2*x1)][float(x5)] 1 18 4.42668e+00 [relu(x2)][x4][x0][x1][x6][sqrt(|x6|)][(x2*x1)] 1 19 4.39144e+00 [relu(x2)][x4][x0][x1][(x6^3)][(x2*x1)] 1 20 4.27968e+00 [relu(x2)][x4][x0][x1][x6][(x6^3)][(x2*x1)] 1 21 4.05328e+00 [relu(x2)][sqrt(|x0|)][x6][x3][x4][(x1*x0)][(x0^2)] 1 22 3.85010e+00 [relu(x2)][sqrt(|x0|)][x6][x3][(x2/x4)][(x1*x0)] 1 23 3.22447e+00 [relu(x2)][x0][sqrt(|x0|)][x6][(x2/x4)][x2][(x1*x0)] 1 24 3.15101e+00 [relu(x2)][x4][x0][sqrt(|x0|)][x6][(x2/x4)][x2][(x1*x0)] 1 26 3.14630e+00 [relu(x2)][x4][x0][sqrt(|x0|)][x6][(x2/x4)][x2][(x1*x0)][flo... 1 28 3.12080e+00 [relu(x2)][x4][x0][x6][(x2/x4)][x2][(x1*x0)][log(x0)] 1 29 3.07627e+00 [relu(x2)][x0][sqrt(|x0|)][x6][(x2/x4)][x2][(x1*x0)][(x6^3)]... 1 32 3.05042e+00 [relu(x2)][x4][x0][x6][(x2/x4)][x2][(x1*x0)][log(x0)][sqrt(|... 1 33 3.04806e+00 [relu(x2)][x4][x3][x0][x6][(x2/x4)][x2][(x1*x0)][log(x0)][sq... 1 38 2.92064e+00 [relu(x2)][x0][sqrt(|x6|)][x6][(x2/x4)][x2][(x1*x0)][log(x0)... 1 45 2.79345e+00 [relu(x2)][x0][x6][(x2/x4)][x2][(x1*x0)][log(x0)][(x6)^(x3)]... 1 46 2.78956e+00 [relu(x2)][x0][x3][x6][(x2/x4)][x2][(x1*x0)][log(x0)][(x6)^(... 1 49 2.74477e+00 [relu(x2)][x0][sqrt(|x6|)][x6][(x2/x4)][x2][(x1*x0)][log(x0)... finished best training representation: [relu(x2)][x0][sqrt(|x6|)][x6][(x2/x4)][x2][(x1*x0)][log(x0)][(x6)^(x3)][(x6^3)] train score: 2.744773 updating best.. best validation representation: [relu(x2)][x0][x6][(x2/x4)][x2][(x1*x0)][log(x0)][(x6)^(x3)][(x6^3)] validation score: 2.907846 final_model score: 2.795017 generating training prediction... train score: 2.7950e+00 train r2: 9.6791e-01 generating test prediction... test score: 2.8997e+00 test r2: 9.7102e-01 printing final model Feature Weight x0 370.995875 (x1*x0) 202.241059 x2 159.910905 relu(x2) -105.321384 log(x0) 101.855423 (x6)^(x3) 85.811398 (x2/x4) 37.755580 x6 35.792342 (x6^3) 25.457528 done!","title":"ENC problem"},{"location":"examples/longitudinal/","text":"This example demonstrates how to do cross validation with longitudinal data. View source Example Patient Data First, we generate some example data and store it. Let s imagine we have patient data from a hospital. This means we have measurements from different visits, with different numbers of measurements from different patients collected in non-uniform intervals. In this example, we make up a classification rule that says that each patient with an increasing body mass index (BMI) and a high maximum glucose level in their blood panel will be assigned as a case (class = 1). import numpy as np import pandas as pd from scipy.stats import rv_continuous , rv_discrete def make_classification ( pdict , data_long ): return a classification # if patient has a max glucose higher than 0.6 and a bmi slope 0, classify true ld = [ d for d in data_long if d [ id ] == pdict [ id ]] bmis = [] dates = [] glucmax = 0 for d in ld : if d [ name ] == bmi : bmis . append ( d [ value ]) dates . append ( d [ date ]) elif d [ name ] == glucose : glucmax = max ( glucmax , d [ value ]) bmis = np . array ( bmis ) dates = np . array ( dates ) bmislope = np . cov ( bmis , dates , ddof = 0 )[ 0 , 1 ] / np . var ( dates ) return 1 if bmislope 0.0 and glucmax 0.8 else 0 if __name__ == __main__ : np . random . seed ( 42 ) # generate data ptid = np . arange ( 1000 ) measurements = [ bmi , age , glucose ] data = [] data_long = [] for p in ptid : # tabular data pdict = {} pdict [ id ] = p pdict [ sex ] = np . random . randint ( 0 , 2 ) pdict [ race ] = np . random . randint ( 0 , 6 ) data . append ( pdict ) # long data age = np . random . randint ( 18 , 85 ) date = np . random . randint ( 1000 , 5000 ) for visit in np . arange ( np . random . randint ( 1 , 20 )): age = age + np . random . randint ( 1 , 4 ) date = date + np . random . randint ( 365 , 3 * 365 ) for m in measurements : plongdict = {} plongdict [ id ] = p plongdict [ name ] = m plongdict [ date ] = date if m == bmi : plongdict [ value ] = int ( visit * np . random . randn ()) + 40 elif m == age : plongdict [ value ] = age elif m == glucose : plongdict [ value ] = np . random . rand () data_long . append ( plongdict ) pdict [ class ] = make_classification ( pdict , data_long ) # np.random.randint(0,2) df = pd . DataFrame . from_records ( data , index = id , columns = [ id , sex , race , class ]) df_long = pd . DataFrame . from_records ( data_long , index = id , columns = [ id , name , date , value ]) df . sort_index ( axis = 0 , inplace = True ) df . to_csv ( d_example_patients.csv ) print ( np . sum ( df [ class ] == 0 ), controls, , np . sum ( df [ class ] == 1 ), cases ) #shuffle rows df_long . to_csv ( d_example_patients_long.csv ) Cross validation Next we set up the learner. We need to declare the longitudinal operators we want to search over. They are defined as a comma-delimited list of strings using the functions argument. In this case, the operators on the second row of the declaration below all operate on longitudinal data. import pandas as pd import numpy as np from feat import Feat from sklearn.model_selection import StratifiedKFold df = pd . read_csv ( d_example_patients.csv ) df . drop ( id , axis = 1 , inplace = True ) X = df . drop ( class , axis = 1 ) . values y = df [ class ] . values zfile = d_example_patients_long.csv kf = StratifiedKFold ( n_splits = 3 ) kf . get_n_splits ( X ) clf = Feat ( max_depth = 5 , max_dim = min ( 50 , 2 * X . shape [ 1 ]), gens = 20 , pop_size = 100 , verbosity = 1 , shuffle = True , ml = LR , classification = True , feature_names = , . join ( df . drop ( class , axis = 1 ) . columns ), functions = +,-,*,/,exp,log,and,or,not,=, , =, , =,ite,split,split_c, mean,median,max,min,variance,skew,kurtosis,slope,count , backprop = True , iters = 10 , random_state = 42 , n_threads = 1 ) Now we train a model using Kfold cross validation. scores = [] for train_idx , test_idx in kf . split ( X , y ): # print( train_idx: ,train_idx) clf . fit ( X [ train_idx ], y [ train_idx ], zfile , train_idx ) scores . append ( clf . score ( X [ test_idx ], y [ test_idx ], zfile , test_idx )) print ( scores: , scores ) The output looks like this: reading d_example_patients_long.csv... read 30717 lines of d_example_patients_long.csv stored 20532 lines, skipped 10185 Completed 100% [======================================================================] reading d_example_patients_long.csv... read 30717 lines of d_example_patients_long.csv stored 10185 lines, skipped 20532 reading d_example_patients_long.csv... read 30717 lines of d_example_patients_long.csv stored 20100 lines, skipped 10617 Completed 100% [======================================================================] reading d_example_patients_long.csv... read 30717 lines of d_example_patients_long.csv stored 10617 lines, skipped 20100 reading d_example_patients_long.csv... read 30717 lines of d_example_patients_long.csv stored 20802 lines, skipped 9915 Completed 100% [======================================================================] reading d_example_patients_long.csv... read 30717 lines of d_example_patients_long.csv stored 9915 lines, skipped 20802 scores: [18.407151361726235, 18.151251136485943, 16.59542398484813] Model Interpretation Now let s fit a model to all the data and try to interpret it. print ( fitting longer to all data... ) clf . gens = 20 clf . verbosity = 2 clf . fit ( X , y , zfile , np . arange ( len ( X ))) The final output shows Generation 19/20 [//////////////////////////////////////////////////] Min Loss Median Loss Median (Max) Size Time (s) 6.81307e-01 6.86632e-01 8 (31) 25.62887 Representation Pareto Front-------------------------------------- Rank Complexity Loss Representation 1 1 7.08447e-01 [sex] 1 2 6.92324e-01 [max(z_glucose)] 1 4 6.91872e-01 [max(z_glucose)][max(z_glucose)] 1 5 6.91332e-01 [(max(z_age)==min(z_bmi))] 1 7 6.90810e-01 [min(z_bmi)][(max(z_age)==min(z_bmi))] 1 10 6.90427e-01 [max(z_glucose)][slope(z_bmi)] 1 12 6.87921e-01 [((median(z_age)-min(z_bmi)) lt;3.889650)] 1 14 6.87917e-01 [min(z_bmi)][((median(z_age)-min(z_bmi)) lt;3.889650)] 1 22 6.85235e-01 [max(z_glucose)][slope(z_bmi)][((median(z_age)-min(z_bmi)) lt;3... 1 24 6.85218e-01 [max(z_glucose)][slope(z_bmi)][min(z_bmi)][((median(z_age)-m... 1 28 6.84598e-01 [slope(z_bmi)][slope(z_bmi)][((median(z_age)-min(z_bmi)) lt;3.8... 1 30 6.84056e-01 [max(z_glucose)][slope(z_bmi)][slope(z_bmi)][((median(z_age)... 1 36 6.83882e-01 [slope(z_bmi)][slope(z_bmi)][slope(z_bmi)][((median(z_age)-m... 1 70 6.83480e-01 [max(z_glucose)][((variance(z_age)/skew(z_bmi)) gt;(kurtosis(z_... 1 72 6.83378e-01 [max(z_glucose)][min(z_bmi)][((variance(z_age)/skew(z_bmi)) gt;... 1 75 6.82466e-01 [min(z_bmi)][((variance(z_age)/skew(z_bmi)) gt;(kurtosis(z_age)... 1 76 6.82049e-01 [slope(z_bmi)][((variance(z_age)/skew(z_bmi)) gt;(kurtosis(z_ag... 1 134 6.81307e-01 [((median(z_age)-min(z_glucose)) lt;-10.132311)][((variance(z_a... finished best training representation: [((median(z_age)-min(z_glucose)) lt;-10.132311)][((variance(z_age)/skew(z_bmi)) gt;(kurtosis(z_age)-skew(z_age)))][((median(z_age)-min(z_bmi)) lt;3.889650)][((slope(z_bmi) lt;-240.605220) AND (sex OR NOT(sex)))] train score: 0.681307 updating best.. best validation representation: [max(z_glucose)][slope(z_bmi)] validation score: 0.690144 final_model score: 0.690150 Here our final representation is composed of slope(z_bmi) and max(z_glucose) , both of which we know to be correct features for this simulated dataset. The best training representation displays clear overfitting, highlighting the importance of using archive validation for model selection. To see the representation again, we run clf.get_representation() , which produces the string [max(z_glucose)][slope(z_bmi)] We can also look at the representation with the model weights, sorted by magnitude, using clf.get_model() : Feature Weight slope(z_bmi) -1.420572 max(z_glucose) -0.705487 Visualizing the representation Here we take the two relevant features and plot the data with them. proj = clf . transform ( X , zfile , np . arange ( len ( X ))) #scale the projection to zero mean, unit deviation from sklearn.preprocessing import StandardScaler proj = StandardScaler () . fit_transform ( proj ) print ( proj: , proj . shape , proj ) # plot import matplotlib.pyplot as plt import seaborn as sns import matplotlib.patheffects as PathEffects # We choose a color palette with seaborn. palette = np . array ( sns . color_palette ( cividis , 2 )) # We create a scatter plot. f = plt . figure ( figsize = ( 8 , 8 )) ax = plt . subplot ( aspect = equal ) sc = ax . scatter ( proj [:, 0 ], proj [:, 1 ], lw = 0 , s = 20 , c = palette [ y . astype ( np . int )]) ax . axis ( square ) # ax.axis( off ) ax . axis ( tight ) # We add the labels for each digit. txts = [] for i in range ( 2 ): # Position of each label. xtext , ytext = np . median ( proj [ y == i , :], axis = 0 ) txt = ax . text ( xtext , ytext , str ( i ), fontsize = 24 ) txt . set_path_effects ([ PathEffects . Stroke ( linewidth = 5 , foreground = w ), PathEffects . Normal ()]) txts . append ( txt ) # add labels from representation rep = [ r . split ( [ )[ - 1 ] for r in clf . get_representation () . split ( ] ) if r != ] print ( rep: , rep ) plt . xlabel ( rep [ 0 ]) plt . ylabel ( rep [ 1 ]) plt . savefig ( longitudinal_representation.svg , dpi = 120 ) This produces the figure below. Data plotted on the axes (slope(z_bmi), max(z_glucose)).","title":"Longitudinal data"},{"location":"examples/longitudinal/#example-patient-data","text":"First, we generate some example data and store it. Let s imagine we have patient data from a hospital. This means we have measurements from different visits, with different numbers of measurements from different patients collected in non-uniform intervals. In this example, we make up a classification rule that says that each patient with an increasing body mass index (BMI) and a high maximum glucose level in their blood panel will be assigned as a case (class = 1). import numpy as np import pandas as pd from scipy.stats import rv_continuous , rv_discrete def make_classification ( pdict , data_long ): return a classification # if patient has a max glucose higher than 0.6 and a bmi slope 0, classify true ld = [ d for d in data_long if d [ id ] == pdict [ id ]] bmis = [] dates = [] glucmax = 0 for d in ld : if d [ name ] == bmi : bmis . append ( d [ value ]) dates . append ( d [ date ]) elif d [ name ] == glucose : glucmax = max ( glucmax , d [ value ]) bmis = np . array ( bmis ) dates = np . array ( dates ) bmislope = np . cov ( bmis , dates , ddof = 0 )[ 0 , 1 ] / np . var ( dates ) return 1 if bmislope 0.0 and glucmax 0.8 else 0 if __name__ == __main__ : np . random . seed ( 42 ) # generate data ptid = np . arange ( 1000 ) measurements = [ bmi , age , glucose ] data = [] data_long = [] for p in ptid : # tabular data pdict = {} pdict [ id ] = p pdict [ sex ] = np . random . randint ( 0 , 2 ) pdict [ race ] = np . random . randint ( 0 , 6 ) data . append ( pdict ) # long data age = np . random . randint ( 18 , 85 ) date = np . random . randint ( 1000 , 5000 ) for visit in np . arange ( np . random . randint ( 1 , 20 )): age = age + np . random . randint ( 1 , 4 ) date = date + np . random . randint ( 365 , 3 * 365 ) for m in measurements : plongdict = {} plongdict [ id ] = p plongdict [ name ] = m plongdict [ date ] = date if m == bmi : plongdict [ value ] = int ( visit * np . random . randn ()) + 40 elif m == age : plongdict [ value ] = age elif m == glucose : plongdict [ value ] = np . random . rand () data_long . append ( plongdict ) pdict [ class ] = make_classification ( pdict , data_long ) # np.random.randint(0,2) df = pd . DataFrame . from_records ( data , index = id , columns = [ id , sex , race , class ]) df_long = pd . DataFrame . from_records ( data_long , index = id , columns = [ id , name , date , value ]) df . sort_index ( axis = 0 , inplace = True ) df . to_csv ( d_example_patients.csv ) print ( np . sum ( df [ class ] == 0 ), controls, , np . sum ( df [ class ] == 1 ), cases ) #shuffle rows df_long . to_csv ( d_example_patients_long.csv )","title":"Example Patient Data"},{"location":"examples/longitudinal/#cross-validation","text":"Next we set up the learner. We need to declare the longitudinal operators we want to search over. They are defined as a comma-delimited list of strings using the functions argument. In this case, the operators on the second row of the declaration below all operate on longitudinal data. import pandas as pd import numpy as np from feat import Feat from sklearn.model_selection import StratifiedKFold df = pd . read_csv ( d_example_patients.csv ) df . drop ( id , axis = 1 , inplace = True ) X = df . drop ( class , axis = 1 ) . values y = df [ class ] . values zfile = d_example_patients_long.csv kf = StratifiedKFold ( n_splits = 3 ) kf . get_n_splits ( X ) clf = Feat ( max_depth = 5 , max_dim = min ( 50 , 2 * X . shape [ 1 ]), gens = 20 , pop_size = 100 , verbosity = 1 , shuffle = True , ml = LR , classification = True , feature_names = , . join ( df . drop ( class , axis = 1 ) . columns ), functions = +,-,*,/,exp,log,and,or,not,=, , =, , =,ite,split,split_c, mean,median,max,min,variance,skew,kurtosis,slope,count , backprop = True , iters = 10 , random_state = 42 , n_threads = 1 ) Now we train a model using Kfold cross validation. scores = [] for train_idx , test_idx in kf . split ( X , y ): # print( train_idx: ,train_idx) clf . fit ( X [ train_idx ], y [ train_idx ], zfile , train_idx ) scores . append ( clf . score ( X [ test_idx ], y [ test_idx ], zfile , test_idx )) print ( scores: , scores ) The output looks like this: reading d_example_patients_long.csv... read 30717 lines of d_example_patients_long.csv stored 20532 lines, skipped 10185 Completed 100% [======================================================================] reading d_example_patients_long.csv... read 30717 lines of d_example_patients_long.csv stored 10185 lines, skipped 20532 reading d_example_patients_long.csv... read 30717 lines of d_example_patients_long.csv stored 20100 lines, skipped 10617 Completed 100% [======================================================================] reading d_example_patients_long.csv... read 30717 lines of d_example_patients_long.csv stored 10617 lines, skipped 20100 reading d_example_patients_long.csv... read 30717 lines of d_example_patients_long.csv stored 20802 lines, skipped 9915 Completed 100% [======================================================================] reading d_example_patients_long.csv... read 30717 lines of d_example_patients_long.csv stored 9915 lines, skipped 20802 scores: [18.407151361726235, 18.151251136485943, 16.59542398484813]","title":"Cross validation"},{"location":"examples/longitudinal/#model-interpretation","text":"Now let s fit a model to all the data and try to interpret it. print ( fitting longer to all data... ) clf . gens = 20 clf . verbosity = 2 clf . fit ( X , y , zfile , np . arange ( len ( X ))) The final output shows Generation 19/20 [//////////////////////////////////////////////////] Min Loss Median Loss Median (Max) Size Time (s) 6.81307e-01 6.86632e-01 8 (31) 25.62887 Representation Pareto Front-------------------------------------- Rank Complexity Loss Representation 1 1 7.08447e-01 [sex] 1 2 6.92324e-01 [max(z_glucose)] 1 4 6.91872e-01 [max(z_glucose)][max(z_glucose)] 1 5 6.91332e-01 [(max(z_age)==min(z_bmi))] 1 7 6.90810e-01 [min(z_bmi)][(max(z_age)==min(z_bmi))] 1 10 6.90427e-01 [max(z_glucose)][slope(z_bmi)] 1 12 6.87921e-01 [((median(z_age)-min(z_bmi)) lt;3.889650)] 1 14 6.87917e-01 [min(z_bmi)][((median(z_age)-min(z_bmi)) lt;3.889650)] 1 22 6.85235e-01 [max(z_glucose)][slope(z_bmi)][((median(z_age)-min(z_bmi)) lt;3... 1 24 6.85218e-01 [max(z_glucose)][slope(z_bmi)][min(z_bmi)][((median(z_age)-m... 1 28 6.84598e-01 [slope(z_bmi)][slope(z_bmi)][((median(z_age)-min(z_bmi)) lt;3.8... 1 30 6.84056e-01 [max(z_glucose)][slope(z_bmi)][slope(z_bmi)][((median(z_age)... 1 36 6.83882e-01 [slope(z_bmi)][slope(z_bmi)][slope(z_bmi)][((median(z_age)-m... 1 70 6.83480e-01 [max(z_glucose)][((variance(z_age)/skew(z_bmi)) gt;(kurtosis(z_... 1 72 6.83378e-01 [max(z_glucose)][min(z_bmi)][((variance(z_age)/skew(z_bmi)) gt;... 1 75 6.82466e-01 [min(z_bmi)][((variance(z_age)/skew(z_bmi)) gt;(kurtosis(z_age)... 1 76 6.82049e-01 [slope(z_bmi)][((variance(z_age)/skew(z_bmi)) gt;(kurtosis(z_ag... 1 134 6.81307e-01 [((median(z_age)-min(z_glucose)) lt;-10.132311)][((variance(z_a... finished best training representation: [((median(z_age)-min(z_glucose)) lt;-10.132311)][((variance(z_age)/skew(z_bmi)) gt;(kurtosis(z_age)-skew(z_age)))][((median(z_age)-min(z_bmi)) lt;3.889650)][((slope(z_bmi) lt;-240.605220) AND (sex OR NOT(sex)))] train score: 0.681307 updating best.. best validation representation: [max(z_glucose)][slope(z_bmi)] validation score: 0.690144 final_model score: 0.690150 Here our final representation is composed of slope(z_bmi) and max(z_glucose) , both of which we know to be correct features for this simulated dataset. The best training representation displays clear overfitting, highlighting the importance of using archive validation for model selection. To see the representation again, we run clf.get_representation() , which produces the string [max(z_glucose)][slope(z_bmi)] We can also look at the representation with the model weights, sorted by magnitude, using clf.get_model() : Feature Weight slope(z_bmi) -1.420572 max(z_glucose) -0.705487","title":"Model Interpretation"},{"location":"examples/longitudinal/#visualizing-the-representation","text":"Here we take the two relevant features and plot the data with them. proj = clf . transform ( X , zfile , np . arange ( len ( X ))) #scale the projection to zero mean, unit deviation from sklearn.preprocessing import StandardScaler proj = StandardScaler () . fit_transform ( proj ) print ( proj: , proj . shape , proj ) # plot import matplotlib.pyplot as plt import seaborn as sns import matplotlib.patheffects as PathEffects # We choose a color palette with seaborn. palette = np . array ( sns . color_palette ( cividis , 2 )) # We create a scatter plot. f = plt . figure ( figsize = ( 8 , 8 )) ax = plt . subplot ( aspect = equal ) sc = ax . scatter ( proj [:, 0 ], proj [:, 1 ], lw = 0 , s = 20 , c = palette [ y . astype ( np . int )]) ax . axis ( square ) # ax.axis( off ) ax . axis ( tight ) # We add the labels for each digit. txts = [] for i in range ( 2 ): # Position of each label. xtext , ytext = np . median ( proj [ y == i , :], axis = 0 ) txt = ax . text ( xtext , ytext , str ( i ), fontsize = 24 ) txt . set_path_effects ([ PathEffects . Stroke ( linewidth = 5 , foreground = w ), PathEffects . Normal ()]) txts . append ( txt ) # add labels from representation rep = [ r . split ( [ )[ - 1 ] for r in clf . get_representation () . split ( ] ) if r != ] print ( rep: , rep ) plt . xlabel ( rep [ 0 ]) plt . ylabel ( rep [ 1 ]) plt . savefig ( longitudinal_representation.svg , dpi = 120 ) This produces the figure below. Data plotted on the axes (slope(z_bmi), max(z_glucose)).","title":"Visualizing the representation"},{"location":"guide/basics/","text":"Feat handles continuous, categorical and boolean data types, as well as sequential (i.e. longitudinal) data. By default, FEAT will attempt to infer these data types automatically from the input data. The user may also specify types in the C++ API. Typical use case For traditional ML tasks, the user specifies data and trains an estimator like so: python from feat import Feat #here s some random data import numpy as np X = np . random . rand ( 100 , 10 ) y = np . random . rand ( 100 ) est = Feat () est . fit ( X , y ) Note that, in python , as in sklearn, FEAT expects X to be an $N \\times D$ numpy array, with $N$ samples and $D$ features. y should be 1d numpy array of length $N$. c++ #include feat.h using FT :: Feat ; #include Eigen/Dense // feed in some data Eigen :: MatrixXd X ( 7 , 2 ); X 0 , 1 , 0.47942554 , 0.87758256 , 0.84147098 , 0.54030231 , 0.99749499 , 0.0707372 , 0.90929743 , - 0.41614684 , 0.59847214 , - 0.80114362 , 0.14112001 , - 0.9899925 ; X . transposeInPlace (); Eigen :: VectorXd y ( 7 ); y 3.0 , 3.59159876 , 3.30384889 , 2.20720158 , 0.57015434 , - 1.20648656 , - 2.68773747 ; // train Feat est ; est . fit ( X , y ); In c++ , FEAT expects X to be transposed, i.e., a $D \\times N$ Eigen MatrixXd . y should be an Eigen VectorXd of length $N$. Command line FEAT can learn from a .csv or .tsv file. In those cases, the target column must be labelled one of the following: class target label To use tab-separated data, specify -sep \\\\t at the command line. If you want to load your own data in a c++ script, you can use Feat s built-in load_csv function. #include feat.h // variables string dataset = my_data.csv ; MatrixXd X ; VectorXd y ; vector string names ; vector char dtypes ; bool binary_endpoint = false ; // true for binary classification char delim = , ; // assuming comma-separated file // load the data FT :: load_csv ( dataset , X , y , names , dtypes , binary_endpoint , delim ); feat . set_feature_names ( names ); feat . set_dtypes ( dtypes ); Now X and y will have your data, and feat will know its types and the names of the variables. Longitudinal data Longitudinal data is handled by passing a file of longitudinal data with an identifier that associates each entry with a row of X . The longitudinal data should have the following format: id date name value 1 365 BloodPressure 128 Each measurement has a unique identifier ( id ), an integer date , a string name , and a value . The ids are used to associate rows of data with rows/samples in X . To do so, the user inputs a numpy array of the same length as X , where each value corresponds to the id value in the longitudinal data associated with that row of X . For example, zfile = longitudinal.csv ids = np . array ([ 1 , ... ]) est . fit ( X , y , zfile , ids ) This means that id=1 associates all data in Z with the first row of X and y . See here for an example.","title":"Basics"},{"location":"guide/basics/#typical-use-case","text":"For traditional ML tasks, the user specifies data and trains an estimator like so: python from feat import Feat #here s some random data import numpy as np X = np . random . rand ( 100 , 10 ) y = np . random . rand ( 100 ) est = Feat () est . fit ( X , y ) Note that, in python , as in sklearn, FEAT expects X to be an $N \\times D$ numpy array, with $N$ samples and $D$ features. y should be 1d numpy array of length $N$. c++ #include feat.h using FT :: Feat ; #include Eigen/Dense // feed in some data Eigen :: MatrixXd X ( 7 , 2 ); X 0 , 1 , 0.47942554 , 0.87758256 , 0.84147098 , 0.54030231 , 0.99749499 , 0.0707372 , 0.90929743 , - 0.41614684 , 0.59847214 , - 0.80114362 , 0.14112001 , - 0.9899925 ; X . transposeInPlace (); Eigen :: VectorXd y ( 7 ); y 3.0 , 3.59159876 , 3.30384889 , 2.20720158 , 0.57015434 , - 1.20648656 , - 2.68773747 ; // train Feat est ; est . fit ( X , y ); In c++ , FEAT expects X to be transposed, i.e., a $D \\times N$ Eigen MatrixXd . y should be an Eigen VectorXd of length $N$.","title":"Typical use case"},{"location":"guide/basics/#command-line","text":"FEAT can learn from a .csv or .tsv file. In those cases, the target column must be labelled one of the following: class target label To use tab-separated data, specify -sep \\\\t at the command line. If you want to load your own data in a c++ script, you can use Feat s built-in load_csv function. #include feat.h // variables string dataset = my_data.csv ; MatrixXd X ; VectorXd y ; vector string names ; vector char dtypes ; bool binary_endpoint = false ; // true for binary classification char delim = , ; // assuming comma-separated file // load the data FT :: load_csv ( dataset , X , y , names , dtypes , binary_endpoint , delim ); feat . set_feature_names ( names ); feat . set_dtypes ( dtypes ); Now X and y will have your data, and feat will know its types and the names of the variables.","title":"Command line"},{"location":"guide/basics/#longitudinal-data","text":"Longitudinal data is handled by passing a file of longitudinal data with an identifier that associates each entry with a row of X . The longitudinal data should have the following format: id date name value 1 365 BloodPressure 128 Each measurement has a unique identifier ( id ), an integer date , a string name , and a value . The ids are used to associate rows of data with rows/samples in X . To do so, the user inputs a numpy array of the same length as X , where each value corresponds to the id value in the longitudinal data associated with that row of X . For example, zfile = longitudinal.csv ids = np . array ([ 1 , ... ]) est . fit ( X , y , zfile , ids ) This means that id=1 associates all data in Z with the first row of X and y . See here for an example.","title":"Longitudinal data"},{"location":"guide/configuration/","text":"Coming soon","title":"Configuration"},{"location":"guide/overview/","text":"This section describes the basic approach used by FEAT. A more detailed description, along with experiments, is available in this preprint. Representation Learning The goal of representation learning in regression or classification is to learn a new representation of your data that makes it easier to model. As an example, consider the figure below 1 , where each point is a sample belonging to one of 4 colored classes. Here, we want to learn the equations on the axes of the right plot (labelled on the axes), which will make it easier classify the data belonging to each class. (Left) raw data. (Right) Data after transformation according to a 2d representation shown on the axes 1 . It s worth noting that the representation in the right panel will be easier for certain machine learning methods to classify, and harder for others. For this reason we ve written FEAT to wrap around the Shogun ML toolbox, which means it can learn representations for different ML approaches. The default approach is linear and logistic regression, but currently decision trees (CART), support vector machines (SVM) and random forests are also supported. Approach FEAT is a wrapper-based learning method that trains ML methods on a population of representations, and optimizes the representations to produce the lowest error. FEAT uses a typical $\\mu$ + $\\lambda$ evolutionary updating scheme, where $\\mu=\\lambda=P$. The method optimizes a population of potential representations, $N = {n_1\\;\\dots\\;n_P}$, where $n$ is an ``individual in the population, iterating through these steps: Fit a linear model $\\hat{y} = \\mathbf{x}^T\\hat{\\beta}$. Create an initial population $N$ consisting of this initial representation, $\\mathbf{\\phi} = \\mathbf{x}$, along with $P-1$ randomly generated representations that sample $\\mathbf{x}$ proportionally to $\\hat{\\beta}$. While the stop criterion is not met: Select parents $P \\subseteq N$ using a selection algorithm. Apply variation operators to parents to generate $P$ offspring $O$; $N = N \\cup O$ Reduce $N$ to $P$ individuals using a survival algorithm. Select and return $n \\in N$ with the lowest error on a hold-out validation set. Individuals are evaluated using an initial forward pass, after which each representation is used to fit a linear model using ridge regression 2 . The weights of the differentiable features in the representation are then updated using stochastic gradient descent. Feature representation FEAT is designed with interpretability in mind. To this end, the representations it learns are sets of equations. The equations are composed of basic operations, including arithmetic, logical functions, control flow and heuristic spits. FEAT also supports many statistical operators for handling sequential data. Selection and Archiving By default, FEAT uses lexicase selection 3 as the selection operation and NSGA-II for survival. This allows FEAT to maintain an archive of accuracy-complexity tradeoffs to aid in interpretability. FEAT also supports simulated annealing, tournament selection and random search. La Cava, W., Silva, S., Danai, K., Spector, L., Vanneschi, L., Moore, J. H. (2018). Multidimensional genetic programming for multiclass classification. Swarm and Evolutionary Computation. Hoerl, A. E., Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1), 55\u201367. La Cava, W., Helmuth, T., Spector, L., Moore, J. H. (2018). A probabilistic and multi-objective analysis of lexicase selection and \u03b5-lexicase selection. Evolutionary computation, 1-28.","title":"Overview"},{"location":"guide/overview/#representation-learning","text":"The goal of representation learning in regression or classification is to learn a new representation of your data that makes it easier to model. As an example, consider the figure below 1 , where each point is a sample belonging to one of 4 colored classes. Here, we want to learn the equations on the axes of the right plot (labelled on the axes), which will make it easier classify the data belonging to each class. (Left) raw data. (Right) Data after transformation according to a 2d representation shown on the axes 1 . It s worth noting that the representation in the right panel will be easier for certain machine learning methods to classify, and harder for others. For this reason we ve written FEAT to wrap around the Shogun ML toolbox, which means it can learn representations for different ML approaches. The default approach is linear and logistic regression, but currently decision trees (CART), support vector machines (SVM) and random forests are also supported.","title":"Representation Learning"},{"location":"guide/overview/#approach","text":"FEAT is a wrapper-based learning method that trains ML methods on a population of representations, and optimizes the representations to produce the lowest error. FEAT uses a typical $\\mu$ + $\\lambda$ evolutionary updating scheme, where $\\mu=\\lambda=P$. The method optimizes a population of potential representations, $N = {n_1\\;\\dots\\;n_P}$, where $n$ is an ``individual in the population, iterating through these steps: Fit a linear model $\\hat{y} = \\mathbf{x}^T\\hat{\\beta}$. Create an initial population $N$ consisting of this initial representation, $\\mathbf{\\phi} = \\mathbf{x}$, along with $P-1$ randomly generated representations that sample $\\mathbf{x}$ proportionally to $\\hat{\\beta}$. While the stop criterion is not met: Select parents $P \\subseteq N$ using a selection algorithm. Apply variation operators to parents to generate $P$ offspring $O$; $N = N \\cup O$ Reduce $N$ to $P$ individuals using a survival algorithm. Select and return $n \\in N$ with the lowest error on a hold-out validation set. Individuals are evaluated using an initial forward pass, after which each representation is used to fit a linear model using ridge regression 2 . The weights of the differentiable features in the representation are then updated using stochastic gradient descent.","title":"Approach"},{"location":"guide/overview/#feature-representation","text":"FEAT is designed with interpretability in mind. To this end, the representations it learns are sets of equations. The equations are composed of basic operations, including arithmetic, logical functions, control flow and heuristic spits. FEAT also supports many statistical operators for handling sequential data.","title":"Feature representation"},{"location":"guide/overview/#selection-and-archiving","text":"By default, FEAT uses lexicase selection 3 as the selection operation and NSGA-II for survival. This allows FEAT to maintain an archive of accuracy-complexity tradeoffs to aid in interpretability. FEAT also supports simulated annealing, tournament selection and random search. La Cava, W., Silva, S., Danai, K., Spector, L., Vanneschi, L., Moore, J. H. (2018). Multidimensional genetic programming for multiclass classification. Swarm and Evolutionary Computation. Hoerl, A. E., Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1), 55\u201367. La Cava, W., Helmuth, T., Spector, L., Moore, J. H. (2018). A probabilistic and multi-objective analysis of lexicase selection and \u03b5-lexicase selection. Evolutionary computation, 1-28.","title":"Selection and Archiving"}]}